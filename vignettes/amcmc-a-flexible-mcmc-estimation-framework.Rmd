---
title: "Workflow with the MCMC function"
author: "George G. Vega Yon"
date: "April 19th, 2019"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Workflow with the MCMC function}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.dim=c(9, 9), out.width=600, out.height=600)
```

We start by loading the dataset that the `mcmc` package includes. We will use the `logit` data set to obtain a posterior distribution of the model parameters using the `MCMC` function.

```{r loading-data, include = TRUE}
library(amcmc)
data(logit, package = "mcmc")
out <- glm(y ~ x1 + x2 + x3 + x4, data = logit, family = binomial, x = TRUE)
beta.init <- as.numeric(coefficients(out))
```

To be able to use the Hastings-Ratio MCMC algorithm the function should be (in principle) the log un-normalized posterior. The following block of code, which was extracted from the `mcmc` package vignette "MCMC Package Example" creates the function that we will be using

```{r log-unnorm-posterior}
lupost_factory <- function(x, y) function(beta) {
    eta <- as.numeric(x %*% beta)
    logp <- ifelse(eta < 0, eta - log1p(exp(eta)), - log1p(exp(- eta)))
    logq <- ifelse(eta < 0, - log1p(exp(eta)), - eta - log1p(exp(- eta)))
    logl <- sum(logp[y == 1]) + sum(logq[y == 0])
    return(logl - sum(beta^2) / 8)
}

lupost <- lupost_factory(out$x, out$y)
```

Let's give it a first try. In this case we will use the beta estimates from the estimated GLM model as a starting point for the algorithm, and we will ask it to sample 1e4 points from the posterior distribution (`nsteps`).

```{r estimation}
# to get reproducible results
set.seed(42)
out <- MCMC(
  lupost,
  initial = beta.init,
  nsteps  = 1e4
)
```

Since the resulting object is of class `mcmc` (from the `coda` R package), we can use all the functions included in `coda` for model diagnostics:

```{r post-estimation}
library(coda)
plot(out[,1:3])
```

The first run doesn't look very good, and furthermore, we should be aware of this since the algorithm did gave us a warning from the convergence cheking component, the chain hasn't converge. Let's give it another try by modifying the scale of the proposal kernel. We can do so as follows:

```{r estimation2}
# to get reproducible results
set.seed(42)    
out <- MCMC(
  lupost,
  initial = beta.init,
  nsteps  = 1e4,
  kernel  = kernel_normal(scale = .5), # Setting the kernel to have a scale =.5
  )
```

Again, let's look at the first three variables of our model:

```{r}
plot(out[,1:3])
```

A bit better. This time, as announced by `MCMC`, the chain has converged to a stationary state, which in principle means that it has forgotten about the starting point. With this, we can rerun the algorithm such that we start from the last couple of step of the chain, this time, without convergence monitoring as it is no longer necesary.

We will increase the number of steps (sample size), use 2 chains using parallel computing:

```{r estimation4}
# Now we change the seed so we get a different stream of
# pseudo random numbers
set.seed(112) 

out_final <- MCMC(
  lupost, 
  initial = tail(out, 1),              # The last 2 points
  nsteps  = 5e4,                       # Increasing the sample size
  kernel  = kernel_normal(scale = .5),
  thin    = 10,
  nchains = 2L,                        # Running parallel chains
  multicore = TRUE,                    # in parallel.
  autostop = 0L                        # No more convergence monitoring
  )
```

Finally, a posterior distribution that looks more like it is actually random

```{r final-results}
plot(out_final[, 1:3])
```


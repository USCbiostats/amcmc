---
title: "Workflow with the MCMC function"
author: "George G. Vega Yon"
date: "April 19th, 2019"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Workflow with the MCMC function}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.dim=c(9, 9), out.width=600, out.height=600)
```

We start by loading the dataset that the `mcmc` package includes. We will use the `logit` data set to obtain a posterior distribution of the model parameters using the `MCMC` function.

```{r loading-data, include = TRUE}
library(amcmc)
data(logit, package = "mcmc")
out <- glm(y ~ x1 + x2 + x3 + x4, data = logit, family = binomial, x = TRUE)
beta.init <- as.numeric(coefficients(out))
```

To be able to use the Metropolis-Hastings MCMC algorithm the function should be (in principle) the log un-normalized posterior. The following block of code, which was extracted from the `mcmc` package vignette "MCMC Package Example" creates the function that we will be using

```{r log-unnorm-posterior}
lupost_factory <- function(x, y) function(beta) {
    eta <- as.numeric(x %*% beta)
    logp <- ifelse(eta < 0, eta - log1p(exp(eta)), - log1p(exp(- eta)))
    logq <- ifelse(eta < 0, - log1p(exp(eta)), - eta - log1p(exp(- eta)))
    logl <- sum(logp[y == 1]) + sum(logq[y == 0])
    return(logl - sum(beta^2) / 8)
}

lupost <- lupost_factory(out$x, out$y)
```

Let's give it a first try. In this case we will use the beta estimates from the estimated GLM model as a starting point for the algorithm, and we will ask it to sample 1e4 points from the posterior distribution (`nsteps`).

```{r estimation}
# to get reproducible results
set.seed(42)
out <- MCMC(
  lupost,
  initial = beta.init,
  nsteps  = 1e4
)
```

Since the resulting object is of class `mcmc` (from the `coda` R package), we can use all the functions included in `coda` for model diagnostics:

```{r post-estimation}
library(coda)
plot(out[,1:3])
```

While the algorithm has converged, the posterior distribution doesn't good at all. Let's give it another try by modifying the scale of the proposal kernel. To do this, we will have to use the `kernel` argument of the function:

```{r estimation2}
# to get reproducible results
set.seed(42)    
out <- MCMC(
  lupost,
  initial = beta.init,
  nsteps  = 1e4,
  kernel  = kernel_normal(scale = .5), # Setting the kernel to have a scale =.5
  )
```

The `kernel_normal`, which is the default kernel in the `MCMC` function, returns an object of class `amcmc_kernel`. In principle it consists on a list of two functions that are used by the `MCMC` routine: `proposal`, the proposal kernel function, and `logratio`, the function that returns the log of the Metropolis-Hastings ratio. We will talk more about `amcmc_kernel` objects later. Now, let's look at the first three variables of our model:

```{r}
plot(out[,1:3])
```

A bit better. As announced by `MCMC`, the chain has reach a stationary state. With this in hand, we can now rerun the algorithm such that we start from the last couple of step of the chain, this time, without convergence monitoring as it is no longer necesary.

We will increase the number of steps (sample size), use 2 chains using parallel computing:

```{r estimation4}
# Now we change the seed so we get a different stream of
# pseudo random numbers
set.seed(112) 

out_final <- MCMC(
  lupost, 
  initial = tail(out, 1),              # The last 2 points
  nsteps  = 5e4,                       # Increasing the sample size
  kernel  = kernel_normal(scale = .5),
  thin    = 10,
  nchains = 2L,                        # Running parallel chains
  multicore = TRUE,                    # in parallel.
  autostop = 0L                        # No more convergence monitoring
  )
```

We now see that the output posterior distribution appears to be stationary

```{r final-results}
plot(out_final[, 1:3])
```

